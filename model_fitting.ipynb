{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanevgenyevich/miniforge3/envs/env_tf/lib/python3.8/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelWithLMHead, \n",
    "    AutoTokenizer, \n",
    "    Trainer,  \n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from huggingface_hub import login\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid.\n",
      "Your token has been saved to /Users/ivanevgenyevich/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login()\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/Users/ivanevgenyevich/miniforge3/envs/env_tf/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:1352: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('tinkoff-ai/ruDialoGPT-medium')\n",
    "model = AutoModelWithLMHead.from_pretrained('tinkoff-ai/ruDialoGPT-medium')\n",
    "\n",
    "def generate(prompt):\n",
    "    data = tokenizer(prompt, return_tensors='pt')\n",
    "    output_ids = model.generate(\n",
    "        **data,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        num_beams=3,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=16,\n",
    "        temperature=1.2,\n",
    "        repetition_penalty=1.2,\n",
    "        length_penalty=1.0,\n",
    "        eos_token_id=50257,\n",
    "        max_new_tokens=40\n",
    "    )[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "    output = output[:output.find('@@')]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanevgenyevich/miniforge3/envs/env_tf/lib/python3.8/site-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type='CAUSAL_LM', \n",
    "    r=64, \n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    target_modules=['c_attn', 'c_proj']\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dialog(dialog):\n",
    "    flag = 0\n",
    "    final_dialog = ''\n",
    "    for message in dialog:\n",
    "        final_dialog += '@@ВТОРОЙ@@ 'if flag else '@@ПЕРВЫЙ@@ '\n",
    "        flag = 1 - flag\n",
    "        final_dialog += message + ' '\n",
    "    return final_dialog    \n",
    "\n",
    "def make_dataset(path):\n",
    "    dataset = pd.read_csv(path)\n",
    "    dialogs = []\n",
    "    len_dataset = dataset.shape[0]\n",
    "    for i in range(len_dataset):\n",
    "        if dataset.loc[i].isna()['context_1']:\n",
    "            if i:\n",
    "                dialogs.append(make_dialog(dialog))\n",
    "            dialog = []\n",
    "        if not dataset.loc[i].isna()['response']:\n",
    "            dialog.append(dataset.loc[i, 'response'])\n",
    "    return dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 681/681 [00:25<00:00, 26.86it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 109.21it/s]\n"
     ]
    }
   ],
   "source": [
    "def convert(dataset):\n",
    "    converter_dataset = []\n",
    "    for i in tqdm(dataset):\n",
    "        input_ids = tokenizer(dataset)['input_ids'][0]\n",
    "        converter_dataset.append({'input_ids': input_ids, 'labels': input_ids})\n",
    "    return converter_dataset\n",
    "\n",
    "dataset = make_dataset('data/mmro.csv') + make_dataset('data/prac.csv') + make_dataset('data/bayes.csv') + make_dataset('data/nlp.csv')[:300]\n",
    "np.random.shuffle(dataset)\n",
    "train_dataset = convert(dataset[:int(0.8 * len(dataset))])\n",
    "eval_dataset = convert(dataset[int(0.8 * len(dataset)):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    num_train_epochs=1,\n",
    "    output_dir='./tg_bot',\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    report_to='wandb',\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=10,\n",
    "    save_steps=10, \n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss'\n",
    ")\n",
    "\n",
    "training_arguments.set_dataloader(pin_memory=False)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "trainer = Trainer(model=model, args=training_arguments, \n",
    "    train_dataset=train_dataset, eval_dataset=eval_dataset, data_collator=data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanevgenyevich/miniforge3/envs/env_tf/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkadchenko-ivan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/ivanevgenyevich/Desktop/sirius/wandb/run-20230908_191736-7c3opdwa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kadchenko-ivan/huggingface/runs/7c3opdwa' target=\"_blank\">bumbling-eon-12</a></strong> to <a href='https://wandb.ai/kadchenko-ivan/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kadchenko-ivan/huggingface' target=\"_blank\">https://wandb.ai/kadchenko-ivan/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kadchenko-ivan/huggingface/runs/7c3opdwa' target=\"_blank\">https://wandb.ai/kadchenko-ivan/huggingface/runs/7c3opdwa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2fa6103ec748a69e2b1027603bbcff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.1235, 'learning_rate': 0.0001, 'epoch': 0.12}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c37f85c44db46869314e5ed0fbc9f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.9384446144104, 'eval_runtime': 11.6274, 'eval_samples_per_second': 14.707, 'eval_steps_per_second': 1.892, 'epoch': 0.12}\n",
      "{'loss': 4.8052, 'learning_rate': 9.578866633275288e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8afb7af4cd348a5815489d3b7f35c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.84628438949585, 'eval_runtime': 12.223, 'eval_samples_per_second': 13.99, 'eval_steps_per_second': 1.8, 'epoch': 0.23}\n",
      "{'loss': 4.1023, 'learning_rate': 8.386407858128706e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10436d8498f4955b8f88ba621d5cbfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.663123607635498, 'eval_runtime': 12.5788, 'eval_samples_per_second': 13.594, 'eval_steps_per_second': 1.749, 'epoch': 0.35}\n",
      "{'loss': 3.192, 'learning_rate': 6.623497346023418e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6fd544ca474652b50271dad90dd023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.454667568206787, 'eval_runtime': 13.0068, 'eval_samples_per_second': 13.147, 'eval_steps_per_second': 1.691, 'epoch': 0.47}\n",
      "{'loss': 2.4781, 'learning_rate': 4.5871032726383386e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "449d32e1c39c419f9f9965fe269f7250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.317712783813477, 'eval_runtime': 12.7423, 'eval_samples_per_second': 13.42, 'eval_steps_per_second': 1.727, 'epoch': 0.58}\n",
      "{'loss': 2.0172, 'learning_rate': 2.6202630348146324e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375d871a2fb04e71a07f60820f0e8b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.245809078216553, 'eval_runtime': 12.3928, 'eval_samples_per_second': 13.798, 'eval_steps_per_second': 1.775, 'epoch': 0.7}\n",
      "{'loss': 1.7127, 'learning_rate': 1.0542974530180327e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19447aca916747fe90b04d4da2c6b6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.214721202850342, 'eval_runtime': 13.0846, 'eval_samples_per_second': 13.069, 'eval_steps_per_second': 1.681, 'epoch': 0.81}\n",
      "{'loss': 1.5714, 'learning_rate': 1.5299867030334814e-06, 'epoch': 0.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfb939c93d54b608cd39e70bd20577f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.207455158233643, 'eval_runtime': 13.016, 'eval_samples_per_second': 13.138, 'eval_steps_per_second': 1.69, 'epoch': 0.93}\n",
      "{'train_runtime': 193.9409, 'train_samples_per_second': 3.511, 'train_steps_per_second': 0.443, 'train_loss': 3.0131957586421523, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(os.path.join('./tg_bot', 'final_checkpoint'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5be94f7a0e424780069c79205be1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056886c014e3443abd6f6928b513a610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/69.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ivankadchenko/my_tg_bot/commit/56d5bdb80c8257fc1ef61f2ad2a791d3387d84b0', commit_message='Upload model', commit_description='', oid='56d5bdb80c8257fc1ef61f2ad2a791d3387d84b0', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('my_tg_bot')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
